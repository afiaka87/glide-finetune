# Simple DDP configuration for multi-GPU training on a single node
# Usage: accelerate launch --config_file configs/accelerate_ddp.yaml train_glide_multi_gpu.py

compute_environment: LOCAL_MACHINE
distributed_type: MULTI_GPU
downcast_bf16: 'no'
gpu_ids: all
machine_rank: 0
main_training_function: main
mixed_precision: 'no'
num_machines: 1
num_processes: auto  # Will use all available GPUs
rdzv_backend: static
same_network: true
tpu_env: []
tpu_use_cluster: false
tpu_use_sudo: false
use_cpu: false