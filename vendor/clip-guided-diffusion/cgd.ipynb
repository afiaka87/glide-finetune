{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "orig_nbformat": 4,
    "language_info": {
      "name": "python",
      "version": "3.9.1",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3.9.1 64-bit ('cgd_venv': venv)"
    },
    "interpreter": {
      "hash": "df369922dce60aef3e878b1b53550d3a55b368f1ac0931e547f318c968180cb0"
    },
    "colab": {
      "name": "cgd.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afiaka87/clip-guided-diffusion/blob/main/cgd.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1Tay1Y169ru"
      },
      "source": [
        "# CLIP Guided Diffusion \n",
        "\n",
        "**From [RiversHaveWings](https://twitter.com/RiversHaveWing)**\n",
        "\n",
        "Generate vibrant and detailed images using only text.\n",
        "- Originally by Katherine Crowson (https://github.com/crowsonkb, https://twitter.com/RiversHaveWings). \n",
        "- Python repository and additions assembled by Clay Mullis (https://github.com/afiaka87)\n",
        "  \n",
        "- **Some amount of programming knowledge is required to use this notebook.**\n",
        "\n",
        "**[Read Me](https://github.com/afiaka87/clip-guided-diffusion)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SsoBNFpL69rv",
        "cellView": "form"
      },
      "source": [
        "#@title Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2021 Clay Mullis, Katherine Crowson\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XnF35z2h69rw",
        "cellView": "form"
      },
      "source": [
        "#@title Grab code and install requirements\n",
        "# colab only\n",
        "import os\n",
        "if 'content' in os.getcwd():\n",
        "    !git clone 'https://github.com/afiaka87/clip-guided-diffusion'\n",
        "    !git clone 'https://github.com/afiaka87/guided-diffusion'\n",
        "    %pip install -r /content/clip-guided-diffusion/requirements.txt \n",
        "    %pip install -e /content/guided-diffusion\n",
        "# !wget --continue 'https://www.dropbox.com/s/476l3ytxh3b7kx1/VIT_B_32_noisy_dallegen_epoch1_step5240.pt' -O vitb32_dalleblog_noise.pt\n",
        "# %pip install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C2l3XbE_q2rp",
        "cellView": "form"
      },
      "source": [
        "#@title Config\n",
        "#@markdown Text Prompts\n",
        "prompts = \"armchair in the form of a pikachu\" #@param{type: \"string\"}\n",
        "image_prompts = \"\" #@param {type: \"string\"}\n",
        "\n",
        "if len(prompts) > 0:\n",
        "    prompts = prompts.split('|')\n",
        "\n",
        "if len(image_prompts) > 0:\n",
        "    image_prompts = image_prompts.split(',')\n",
        "\n",
        "#@markdown CLIP/Diffusion\n",
        "image_size = \"128\"  #@param [\"64\", \"128\", \"256\", \"512\"]\n",
        "image_size = int(image_size)\n",
        "unconditional = False #@param{type: \"boolean\"}\n",
        "clip_model_name = \"ViT-B/32\" #@param [\"RN50\", \"ViT-B/32\", \"ViT-B/16\", \"RN50x4\", \"RN50x16\", \"vitb32_dalleblog_noise.pt\"] {allow-input: true}\n",
        "\n",
        "#@markdown Initial image (url or path, set to \"\" to disable)\n",
        "init_image = \"\" #@param{type: \"string\"}\n",
        "init_scale = 0 #@param {type: \"number\"}\n",
        "skip_timesteps =  0#@param{type: \"number\"}\n",
        "if len(init_image) > 0 and skip_timesteps == 0:\n",
        "    print(\"Overriding skip_timesteps to 500 because init_image is set but skip_timesteps is 0. You may want to override this manually.\")\n",
        "    skip_timesteps = 500\n",
        "if len(init_image) == 0:\n",
        "    if skip_timesteps > 0:\n",
        "        print(\"Overriding skip_timesteps to 0 because you dont have an init image set.\")\n",
        "        skip_timesteps = 0\n",
        "\n",
        "#@markdown Knobs\n",
        "num_cutouts = 32 #@param{type: \"number\"}\n",
        "timestep_respacing = \"250\" #@param [\"25\", \"50\", \"100\", \"250\", \"500\", \"1000\", \"ddim25\", \"ddim50\", \"ddim100\", \"ddim250\", \"ddim500\", \"ddim1000\"]\n",
        "clip_guidance_scale =  1500 #@param{type: \"number\"}\n",
        "tv_scale =  150 #@param{type: \"number\"}\n",
        "\n",
        "range_scale =  50 #@param {type: \"number\"}\n",
        "cutout_power = 0.5 #@param {type: \"number\"}\n",
        "cutout_power = float(cutout_power)\n",
        "dropout = 0.1 if image_size == 64 else 0.0\n",
        "num_classes =  0#@param {type: \"number\"}\n",
        "#@markdown Output\n",
        "save_frequency = 5 #@param{type: \"number\"}\n",
        "\n",
        "#@markdown \"diffusion_steps\" schedule of timesteps over which to run diffusion. Values other than 1000 can lead to NaN errors.\n",
        "diffusion_steps = 1000 #@param {type: \"number\"}\n",
        "\n",
        "batch_size = 1 #@param {type: \"number\"}\n",
        "\n",
        "random_affine = False #@param{type: \"boolean\"}\n",
        "random_motion_blur = False #@param{type: \"boolean\"}\n",
        "random_horizontal_flip = False #@param{type: \"boolean\"}\n",
        "\n",
        "seed = 0 #@param {type: \"number\"}\n",
        "\n",
        "# probably don't touch\n",
        "noise_schedule = \"cosine\" if image_size == 64 else \"linear\"\n",
        "output_dir = \"outputs\"\n",
        "augs = []\n",
        "import kornia.augmentation as kaugs\n",
        "if random_affine: augs.append(kaugs.RandomAffine(degrees=15, translate=(0.1, 0.1), scale=(0.9, 1.1), shear=0.1))\n",
        "if random_motion_blur: augs.append(kaugs.RandomMotionBlur(kernel_size=(3, 5), angle=15, direction=0.5))\n",
        "if random_horizontal_flip: augs.append(kaugs.RandomHorizontalFlip(p=0.5))\n",
        "import torch\n",
        "from torchvision import transforms as T\n",
        "# augs.append(T.RandomApply([T.Lambda(lambda x : x + torch.randn_like(x)),], p=0.5))\n",
        "total_steps = int(timestep_respacing.replace(\"ddim\",\"\")) - skip_timesteps\n",
        "\n",
        "# Load init image.\n",
        "if len(init_image) > 0:\n",
        "    if init_image.startswith(\"http\"):\n",
        "        !wget --continue $init_image -O init_image.jpg\n",
        "    else:\n",
        "        !cp -n -v $init_image init_image.jpg\n",
        "    init_image = \"init_image.jpg\"\n",
        "_class_cond = not unconditional\n",
        "\n",
        "from IPython.core.interactiveshell import InteractiveShell\n",
        "InteractiveShell.ast_node_interactivity = \"all\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fvtIw5G9q2rq"
      },
      "source": [
        "# Run"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UhCsj4q8q2rr",
        "cellView": "form"
      },
      "source": [
        "#@title Imports/Helper functions\n",
        "import sys\n",
        "sys.path.append(\"clip-guided-diffusion\")\n",
        "sys.path.append(\"guided-diffusion\")\n",
        "import pathlib\n",
        "from IPython.display import Image as ipy_Image\n",
        "from IPython.display import display as ipy_display\n",
        "from IPython.display import clear_output as ipy_clear_output\n",
        "from tqdm.autonotebook import tqdm\n",
        "import torch as th\n",
        "import base64\n",
        "from PIL import Image as pil_Image\n",
        "from IPython.display import IFrame\n",
        "import os\n",
        "import torch\n",
        "from cgd.util import create_gif\n",
        "\n",
        "from IPython.display import HTML\n",
        "import base64\n",
        "\n",
        "def show_gif(fname):\n",
        "    with open(fname, 'rb') as fd:\n",
        "        b64 = base64.b64encode(fd.read()).decode('ascii')\n",
        "    return HTML(f'<img src=\"data:image/gif;base64,{b64}\" />')\n",
        "\n",
        "# thanks @nshepperd!\n",
        "def refresh_cuda_memory():\n",
        "    import gc\n",
        "    gc.collect()\n",
        "    for obj in gc.get_objects():\n",
        "        if not isinstance(obj, th.Tensor):\n",
        "            continue\n",
        "        if isinstance(obj, th.nn.Parameter):\n",
        "            obj.grad = None\n",
        "            continue\n",
        "\n",
        "        obj.data = obj.data.cpu()\n",
        "    gc.collect()\n",
        "    th.cuda.empty_cache()\n",
        "refresh_cuda_memory()\n",
        "\n",
        "pathlib.Path(\"outputs\").mkdir(exist_ok=True)\n",
        "total_steps = int(timestep_respacing.replace(\"ddim\",\"\")) - skip_timesteps\n",
        "progress_bar = tqdm(total=total_steps, unit=\"steps\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpnKJ7RNw4W0",
        "cellView": "form"
      },
      "source": [
        "#@title Start diffusion\n",
        "\n",
        "clear_scrollback = True # @param {type:\"boolean\"}\n",
        "display_width = \"\" \n",
        "if len(display_width) > 0:\n",
        "    display_width = int(display_width)\n",
        "else:\n",
        "    display_width = 256 if image_size < 512 else 512\n",
        "\n",
        "from cgd.cgd import clip_guided_diffusion\n",
        "cgd_samples = clip_guided_diffusion(\n",
        "    prompts=prompts,\n",
        "    image_prompts=image_prompts,\n",
        "    batch_size=batch_size,\n",
        "    tv_scale=tv_scale,\n",
        "    range_scale=range_scale,\n",
        "    image_size=image_size,\n",
        "    class_cond=_class_cond,\n",
        "    clip_guidance_scale=clip_guidance_scale,\n",
        "    cutout_power=cutout_power,\n",
        "    num_cutouts=num_cutouts,\n",
        "    timestep_respacing=timestep_respacing,\n",
        "    seed=seed,\n",
        "    diffusion_steps=diffusion_steps,\n",
        "    skip_timesteps=skip_timesteps,\n",
        "    init_image=init_image,\n",
        "    init_scale=init_scale,\n",
        "    clip_model_name=clip_model_name,\n",
        "    randomize_class=(_class_cond),\n",
        "    noise_schedule=noise_schedule,\n",
        "    dropout=dropout,\n",
        "    num_classes=num_classes,\n",
        "    augs=augs\n",
        ")\n",
        "for batch_idx, img in cgd_samples:\n",
        "    if clear_scrollback and batch_idx == 0:\n",
        "        ipy_clear_output(wait=True)\n",
        "    progress_bar.set_description(f\"Saved image {img}\")\n",
        "    progress_bar.update()\n",
        "    ipy_display(ipy_Image(img, width=display_width))\n",
        "\n",
        "gif_paths = [ create_gif(base='outputs',prompts=prompts,batch_idx=batch_idx) for batch_idx in range(batch_size) ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ed6h1OOtw4W1",
        "cellView": "form"
      },
      "source": [
        "#@title Display gifs\n",
        "def gif_html_img_str(gif_path):\n",
        "    with open(gif_path, 'rb') as fd:\n",
        "        b64 = base64.b64encode(fd.read()).decode('ascii')\n",
        "    return f'<img src=\"data:image/gif;base64,{b64}\" />'\n",
        "\n",
        "\n",
        "for gif_path in gif_paths:\n",
        "    ipy_display(HTML(gif_html_img_str(gif_path)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}